{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "cartoongan-impl.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1WgQ1lxkpkbLccBITQUsaO6k2J6JWt2ww",
      "authorship_tag": "ABX9TyPymUp5hliswhwH8eRU3OC4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GarlandZhang/pg-toons/blob/master/cartoongan_impl.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bmDVaw7HqWmD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "f0220dba-c949-4afa-a833-ef6b9e8c74bf"
      },
      "source": [
        "!rm -r keras-contrib/ \n",
        "# !pip uninstall -y tensorflow-gpu==2.0.0-alpha0\n",
        "# !pip uninstall -y tensorflow\n",
        "# !pip install tensorflow-gpu==2.0.0-alpha0\n",
        "!git clone https://www.github.com/keras-team/keras-contrib.git \\\n",
        "    && cd keras-contrib \\\n",
        "    && pip install git+https://www.github.com/keras-team/keras-contrib.git \\\n",
        "    && python convert_to_tf_keras.py \\\n",
        "    && USE_TF_KERAS=1 python setup.py install\n",
        "# clear_output()\n",
        "!pip install -q  --no-deps tensorflow-addons~=0.7"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "rm: cannot remove 'keras-contrib/': No such file or directory\n",
            "Cloning into 'keras-contrib'...\n",
            "warning: redirecting to https://github.com/keras-team/keras-contrib.git/\n",
            "remote: Enumerating objects: 3634, done.\u001b[K\n",
            "remote: Total 3634 (delta 0), reused 0 (delta 0), pack-reused 3634\u001b[K\n",
            "Receiving objects: 100% (3634/3634), 861.24 KiB | 13.46 MiB/s, done.\n",
            "Resolving deltas: 100% (2330/2330), done.\n",
            "Collecting git+https://www.github.com/keras-team/keras-contrib.git\n",
            "  Cloning https://www.github.com/keras-team/keras-contrib.git to /tmp/pip-req-build-5buyeidp\n",
            "  Running command git clone -q https://www.github.com/keras-team/keras-contrib.git /tmp/pip-req-build-5buyeidp\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.6/dist-packages (from keras-contrib==2.0.8) (2.3.1)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from keras->keras-contrib==2.0.8) (1.1.2)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from keras->keras-contrib==2.0.8) (1.0.8)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras->keras-contrib==2.0.8) (2.10.0)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from keras->keras-contrib==2.0.8) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from keras->keras-contrib==2.0.8) (1.18.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras->keras-contrib==2.0.8) (3.13)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from keras->keras-contrib==2.0.8) (1.12.0)\n",
            "Building wheels for collected packages: keras-contrib\n",
            "  Building wheel for keras-contrib (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-contrib: filename=keras_contrib-2.0.8-cp36-none-any.whl size=101064 sha256=99f05b2310d8f35aa3cd5f59342ab9fb34d6d78bfcfc7b7ad15d575883f4ae31\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-yjig_9nb/wheels/11/27/c8/4ed56de7b55f4f61244e2dc6ef3cdbaff2692527a2ce6502ba\n",
            "Successfully built keras-contrib\n",
            "Installing collected packages: keras-contrib\n",
            "Successfully installed keras-contrib-2.0.8\n",
            "Changed imports in 73 files.\n",
            "Those files were found in the directory /content/keras-contrib\n",
            "running install\n",
            "running bdist_egg\n",
            "running egg_info\n",
            "creating tf_keras_contrib.egg-info\n",
            "writing tf_keras_contrib.egg-info/PKG-INFO\n",
            "writing dependency_links to tf_keras_contrib.egg-info/dependency_links.txt\n",
            "writing requirements to tf_keras_contrib.egg-info/requires.txt\n",
            "writing top-level names to tf_keras_contrib.egg-info/top_level.txt\n",
            "writing manifest file 'tf_keras_contrib.egg-info/SOURCES.txt'\n",
            "writing manifest file 'tf_keras_contrib.egg-info/SOURCES.txt'\n",
            "installing library code to build/bdist.linux-x86_64/egg\n",
            "running install_lib\n",
            "running build_py\n",
            "creating build\n",
            "creating build/lib\n",
            "creating build/lib/keras_contrib\n",
            "copying keras_contrib/__init__.py -> build/lib/keras_contrib\n",
            "creating build/lib/keras_contrib/utils\n",
            "copying keras_contrib/utils/test_utils.py -> build/lib/keras_contrib/utils\n",
            "copying keras_contrib/utils/__init__.py -> build/lib/keras_contrib/utils\n",
            "copying keras_contrib/utils/save_load_utils.py -> build/lib/keras_contrib/utils\n",
            "copying keras_contrib/utils/conv_utils.py -> build/lib/keras_contrib/utils\n",
            "creating build/lib/keras_contrib/losses\n",
            "copying keras_contrib/losses/__init__.py -> build/lib/keras_contrib/losses\n",
            "copying keras_contrib/losses/jaccard.py -> build/lib/keras_contrib/losses\n",
            "copying keras_contrib/losses/dssim.py -> build/lib/keras_contrib/losses\n",
            "copying keras_contrib/losses/crf_losses.py -> build/lib/keras_contrib/losses\n",
            "creating build/lib/keras_contrib/metrics\n",
            "copying keras_contrib/metrics/crf_accuracies.py -> build/lib/keras_contrib/metrics\n",
            "copying keras_contrib/metrics/__init__.py -> build/lib/keras_contrib/metrics\n",
            "creating build/lib/keras_contrib/optimizers\n",
            "copying keras_contrib/optimizers/yogi.py -> build/lib/keras_contrib/optimizers\n",
            "copying keras_contrib/optimizers/__init__.py -> build/lib/keras_contrib/optimizers\n",
            "copying keras_contrib/optimizers/padam.py -> build/lib/keras_contrib/optimizers\n",
            "copying keras_contrib/optimizers/ftml.py -> build/lib/keras_contrib/optimizers\n",
            "copying keras_contrib/optimizers/lars.py -> build/lib/keras_contrib/optimizers\n",
            "creating build/lib/keras_contrib/preprocessing\n",
            "copying keras_contrib/preprocessing/__init__.py -> build/lib/keras_contrib/preprocessing\n",
            "creating build/lib/keras_contrib/regularizers\n",
            "copying keras_contrib/regularizers/__init__.py -> build/lib/keras_contrib/regularizers\n",
            "creating build/lib/keras_contrib/constraints\n",
            "copying keras_contrib/constraints/clip.py -> build/lib/keras_contrib/constraints\n",
            "copying keras_contrib/constraints/__init__.py -> build/lib/keras_contrib/constraints\n",
            "creating build/lib/keras_contrib/activations\n",
            "copying keras_contrib/activations/squash.py -> build/lib/keras_contrib/activations\n",
            "copying keras_contrib/activations/__init__.py -> build/lib/keras_contrib/activations\n",
            "creating build/lib/keras_contrib/datasets\n",
            "copying keras_contrib/datasets/__init__.py -> build/lib/keras_contrib/datasets\n",
            "copying keras_contrib/datasets/conll2000.py -> build/lib/keras_contrib/datasets\n",
            "copying keras_contrib/datasets/pascal_voc.py -> build/lib/keras_contrib/datasets\n",
            "copying keras_contrib/datasets/coco.py -> build/lib/keras_contrib/datasets\n",
            "creating build/lib/keras_contrib/initializers\n",
            "copying keras_contrib/initializers/__init__.py -> build/lib/keras_contrib/initializers\n",
            "copying keras_contrib/initializers/convaware.py -> build/lib/keras_contrib/initializers\n",
            "creating build/lib/keras_contrib/tests\n",
            "copying keras_contrib/tests/activations.py -> build/lib/keras_contrib/tests\n",
            "copying keras_contrib/tests/__init__.py -> build/lib/keras_contrib/tests\n",
            "copying keras_contrib/tests/optimizers.py -> build/lib/keras_contrib/tests\n",
            "copying keras_contrib/tests/metrics.py -> build/lib/keras_contrib/tests\n",
            "copying keras_contrib/tests/regularizers.py -> build/lib/keras_contrib/tests\n",
            "creating build/lib/keras_contrib/layers\n",
            "copying keras_contrib/layers/__init__.py -> build/lib/keras_contrib/layers\n",
            "copying keras_contrib/layers/core.py -> build/lib/keras_contrib/layers\n",
            "copying keras_contrib/layers/crf.py -> build/lib/keras_contrib/layers\n",
            "copying keras_contrib/layers/capsule.py -> build/lib/keras_contrib/layers\n",
            "creating build/lib/keras_contrib/applications\n",
            "copying keras_contrib/applications/densenet.py -> build/lib/keras_contrib/applications\n",
            "copying keras_contrib/applications/__init__.py -> build/lib/keras_contrib/applications\n",
            "copying keras_contrib/applications/resnet.py -> build/lib/keras_contrib/applications\n",
            "copying keras_contrib/applications/nasnet.py -> build/lib/keras_contrib/applications\n",
            "copying keras_contrib/applications/wide_resnet.py -> build/lib/keras_contrib/applications\n",
            "creating build/lib/keras_contrib/wrappers\n",
            "copying keras_contrib/wrappers/__init__.py -> build/lib/keras_contrib/wrappers\n",
            "creating build/lib/keras_contrib/callbacks\n",
            "copying keras_contrib/callbacks/cyclical_learning_rate.py -> build/lib/keras_contrib/callbacks\n",
            "copying keras_contrib/callbacks/__init__.py -> build/lib/keras_contrib/callbacks\n",
            "copying keras_contrib/callbacks/dead_relu_detector.py -> build/lib/keras_contrib/callbacks\n",
            "copying keras_contrib/callbacks/tensorboard.py -> build/lib/keras_contrib/callbacks\n",
            "copying keras_contrib/callbacks/snapshot.py -> build/lib/keras_contrib/callbacks\n",
            "creating build/lib/keras_contrib/backend\n",
            "copying keras_contrib/backend/cntk_backend.py -> build/lib/keras_contrib/backend\n",
            "copying keras_contrib/backend/tensorflow_backend.py -> build/lib/keras_contrib/backend\n",
            "copying keras_contrib/backend/__init__.py -> build/lib/keras_contrib/backend\n",
            "copying keras_contrib/backend/numpy_backend.py -> build/lib/keras_contrib/backend\n",
            "copying keras_contrib/backend/theano_backend.py -> build/lib/keras_contrib/backend\n",
            "creating build/lib/keras_contrib/layers/advanced_activations\n",
            "copying keras_contrib/layers/advanced_activations/swish.py -> build/lib/keras_contrib/layers/advanced_activations\n",
            "copying keras_contrib/layers/advanced_activations/sinerelu.py -> build/lib/keras_contrib/layers/advanced_activations\n",
            "copying keras_contrib/layers/advanced_activations/srelu.py -> build/lib/keras_contrib/layers/advanced_activations\n",
            "copying keras_contrib/layers/advanced_activations/__init__.py -> build/lib/keras_contrib/layers/advanced_activations\n",
            "copying keras_contrib/layers/advanced_activations/pelu.py -> build/lib/keras_contrib/layers/advanced_activations\n",
            "creating build/lib/keras_contrib/layers/normalization\n",
            "copying keras_contrib/layers/normalization/instancenormalization.py -> build/lib/keras_contrib/layers/normalization\n",
            "copying keras_contrib/layers/normalization/__init__.py -> build/lib/keras_contrib/layers/normalization\n",
            "copying keras_contrib/layers/normalization/groupnormalization.py -> build/lib/keras_contrib/layers/normalization\n",
            "creating build/lib/keras_contrib/layers/convolutional\n",
            "copying keras_contrib/layers/convolutional/__init__.py -> build/lib/keras_contrib/layers/convolutional\n",
            "copying keras_contrib/layers/convolutional/cosineconvolution2d.py -> build/lib/keras_contrib/layers/convolutional\n",
            "copying keras_contrib/layers/convolutional/subpixelupscaling.py -> build/lib/keras_contrib/layers/convolutional\n",
            "creating build/bdist.linux-x86_64\n",
            "creating build/bdist.linux-x86_64/egg\n",
            "creating build/bdist.linux-x86_64/egg/keras_contrib\n",
            "creating build/bdist.linux-x86_64/egg/keras_contrib/utils\n",
            "copying build/lib/keras_contrib/utils/test_utils.py -> build/bdist.linux-x86_64/egg/keras_contrib/utils\n",
            "copying build/lib/keras_contrib/utils/__init__.py -> build/bdist.linux-x86_64/egg/keras_contrib/utils\n",
            "copying build/lib/keras_contrib/utils/save_load_utils.py -> build/bdist.linux-x86_64/egg/keras_contrib/utils\n",
            "copying build/lib/keras_contrib/utils/conv_utils.py -> build/bdist.linux-x86_64/egg/keras_contrib/utils\n",
            "creating build/bdist.linux-x86_64/egg/keras_contrib/losses\n",
            "copying build/lib/keras_contrib/losses/__init__.py -> build/bdist.linux-x86_64/egg/keras_contrib/losses\n",
            "copying build/lib/keras_contrib/losses/jaccard.py -> build/bdist.linux-x86_64/egg/keras_contrib/losses\n",
            "copying build/lib/keras_contrib/losses/dssim.py -> build/bdist.linux-x86_64/egg/keras_contrib/losses\n",
            "copying build/lib/keras_contrib/losses/crf_losses.py -> build/bdist.linux-x86_64/egg/keras_contrib/losses\n",
            "creating build/bdist.linux-x86_64/egg/keras_contrib/metrics\n",
            "copying build/lib/keras_contrib/metrics/crf_accuracies.py -> build/bdist.linux-x86_64/egg/keras_contrib/metrics\n",
            "copying build/lib/keras_contrib/metrics/__init__.py -> build/bdist.linux-x86_64/egg/keras_contrib/metrics\n",
            "creating build/bdist.linux-x86_64/egg/keras_contrib/optimizers\n",
            "copying build/lib/keras_contrib/optimizers/yogi.py -> build/bdist.linux-x86_64/egg/keras_contrib/optimizers\n",
            "copying build/lib/keras_contrib/optimizers/__init__.py -> build/bdist.linux-x86_64/egg/keras_contrib/optimizers\n",
            "copying build/lib/keras_contrib/optimizers/padam.py -> build/bdist.linux-x86_64/egg/keras_contrib/optimizers\n",
            "copying build/lib/keras_contrib/optimizers/ftml.py -> build/bdist.linux-x86_64/egg/keras_contrib/optimizers\n",
            "copying build/lib/keras_contrib/optimizers/lars.py -> build/bdist.linux-x86_64/egg/keras_contrib/optimizers\n",
            "creating build/bdist.linux-x86_64/egg/keras_contrib/preprocessing\n",
            "copying build/lib/keras_contrib/preprocessing/__init__.py -> build/bdist.linux-x86_64/egg/keras_contrib/preprocessing\n",
            "creating build/bdist.linux-x86_64/egg/keras_contrib/regularizers\n",
            "copying build/lib/keras_contrib/regularizers/__init__.py -> build/bdist.linux-x86_64/egg/keras_contrib/regularizers\n",
            "creating build/bdist.linux-x86_64/egg/keras_contrib/constraints\n",
            "copying build/lib/keras_contrib/constraints/clip.py -> build/bdist.linux-x86_64/egg/keras_contrib/constraints\n",
            "copying build/lib/keras_contrib/constraints/__init__.py -> build/bdist.linux-x86_64/egg/keras_contrib/constraints\n",
            "copying build/lib/keras_contrib/__init__.py -> build/bdist.linux-x86_64/egg/keras_contrib\n",
            "creating build/bdist.linux-x86_64/egg/keras_contrib/activations\n",
            "copying build/lib/keras_contrib/activations/squash.py -> build/bdist.linux-x86_64/egg/keras_contrib/activations\n",
            "copying build/lib/keras_contrib/activations/__init__.py -> build/bdist.linux-x86_64/egg/keras_contrib/activations\n",
            "creating build/bdist.linux-x86_64/egg/keras_contrib/datasets\n",
            "copying build/lib/keras_contrib/datasets/__init__.py -> build/bdist.linux-x86_64/egg/keras_contrib/datasets\n",
            "copying build/lib/keras_contrib/datasets/conll2000.py -> build/bdist.linux-x86_64/egg/keras_contrib/datasets\n",
            "copying build/lib/keras_contrib/datasets/pascal_voc.py -> build/bdist.linux-x86_64/egg/keras_contrib/datasets\n",
            "copying build/lib/keras_contrib/datasets/coco.py -> build/bdist.linux-x86_64/egg/keras_contrib/datasets\n",
            "creating build/bdist.linux-x86_64/egg/keras_contrib/initializers\n",
            "copying build/lib/keras_contrib/initializers/__init__.py -> build/bdist.linux-x86_64/egg/keras_contrib/initializers\n",
            "copying build/lib/keras_contrib/initializers/convaware.py -> build/bdist.linux-x86_64/egg/keras_contrib/initializers\n",
            "creating build/bdist.linux-x86_64/egg/keras_contrib/tests\n",
            "copying build/lib/keras_contrib/tests/activations.py -> build/bdist.linux-x86_64/egg/keras_contrib/tests\n",
            "copying build/lib/keras_contrib/tests/__init__.py -> build/bdist.linux-x86_64/egg/keras_contrib/tests\n",
            "copying build/lib/keras_contrib/tests/optimizers.py -> build/bdist.linux-x86_64/egg/keras_contrib/tests\n",
            "copying build/lib/keras_contrib/tests/metrics.py -> build/bdist.linux-x86_64/egg/keras_contrib/tests\n",
            "copying build/lib/keras_contrib/tests/regularizers.py -> build/bdist.linux-x86_64/egg/keras_contrib/tests\n",
            "creating build/bdist.linux-x86_64/egg/keras_contrib/layers\n",
            "creating build/bdist.linux-x86_64/egg/keras_contrib/layers/advanced_activations\n",
            "copying build/lib/keras_contrib/layers/advanced_activations/swish.py -> build/bdist.linux-x86_64/egg/keras_contrib/layers/advanced_activations\n",
            "copying build/lib/keras_contrib/layers/advanced_activations/sinerelu.py -> build/bdist.linux-x86_64/egg/keras_contrib/layers/advanced_activations\n",
            "copying build/lib/keras_contrib/layers/advanced_activations/srelu.py -> build/bdist.linux-x86_64/egg/keras_contrib/layers/advanced_activations\n",
            "copying build/lib/keras_contrib/layers/advanced_activations/__init__.py -> build/bdist.linux-x86_64/egg/keras_contrib/layers/advanced_activations\n",
            "copying build/lib/keras_contrib/layers/advanced_activations/pelu.py -> build/bdist.linux-x86_64/egg/keras_contrib/layers/advanced_activations\n",
            "copying build/lib/keras_contrib/layers/__init__.py -> build/bdist.linux-x86_64/egg/keras_contrib/layers\n",
            "creating build/bdist.linux-x86_64/egg/keras_contrib/layers/normalization\n",
            "copying build/lib/keras_contrib/layers/normalization/instancenormalization.py -> build/bdist.linux-x86_64/egg/keras_contrib/layers/normalization\n",
            "copying build/lib/keras_contrib/layers/normalization/__init__.py -> build/bdist.linux-x86_64/egg/keras_contrib/layers/normalization\n",
            "copying build/lib/keras_contrib/layers/normalization/groupnormalization.py -> build/bdist.linux-x86_64/egg/keras_contrib/layers/normalization\n",
            "copying build/lib/keras_contrib/layers/core.py -> build/bdist.linux-x86_64/egg/keras_contrib/layers\n",
            "copying build/lib/keras_contrib/layers/crf.py -> build/bdist.linux-x86_64/egg/keras_contrib/layers\n",
            "copying build/lib/keras_contrib/layers/capsule.py -> build/bdist.linux-x86_64/egg/keras_contrib/layers\n",
            "creating build/bdist.linux-x86_64/egg/keras_contrib/layers/convolutional\n",
            "copying build/lib/keras_contrib/layers/convolutional/__init__.py -> build/bdist.linux-x86_64/egg/keras_contrib/layers/convolutional\n",
            "copying build/lib/keras_contrib/layers/convolutional/cosineconvolution2d.py -> build/bdist.linux-x86_64/egg/keras_contrib/layers/convolutional\n",
            "copying build/lib/keras_contrib/layers/convolutional/subpixelupscaling.py -> build/bdist.linux-x86_64/egg/keras_contrib/layers/convolutional\n",
            "creating build/bdist.linux-x86_64/egg/keras_contrib/applications\n",
            "copying build/lib/keras_contrib/applications/densenet.py -> build/bdist.linux-x86_64/egg/keras_contrib/applications\n",
            "copying build/lib/keras_contrib/applications/__init__.py -> build/bdist.linux-x86_64/egg/keras_contrib/applications\n",
            "copying build/lib/keras_contrib/applications/resnet.py -> build/bdist.linux-x86_64/egg/keras_contrib/applications\n",
            "copying build/lib/keras_contrib/applications/nasnet.py -> build/bdist.linux-x86_64/egg/keras_contrib/applications\n",
            "copying build/lib/keras_contrib/applications/wide_resnet.py -> build/bdist.linux-x86_64/egg/keras_contrib/applications\n",
            "creating build/bdist.linux-x86_64/egg/keras_contrib/wrappers\n",
            "copying build/lib/keras_contrib/wrappers/__init__.py -> build/bdist.linux-x86_64/egg/keras_contrib/wrappers\n",
            "creating build/bdist.linux-x86_64/egg/keras_contrib/callbacks\n",
            "copying build/lib/keras_contrib/callbacks/cyclical_learning_rate.py -> build/bdist.linux-x86_64/egg/keras_contrib/callbacks\n",
            "copying build/lib/keras_contrib/callbacks/__init__.py -> build/bdist.linux-x86_64/egg/keras_contrib/callbacks\n",
            "copying build/lib/keras_contrib/callbacks/dead_relu_detector.py -> build/bdist.linux-x86_64/egg/keras_contrib/callbacks\n",
            "copying build/lib/keras_contrib/callbacks/tensorboard.py -> build/bdist.linux-x86_64/egg/keras_contrib/callbacks\n",
            "copying build/lib/keras_contrib/callbacks/snapshot.py -> build/bdist.linux-x86_64/egg/keras_contrib/callbacks\n",
            "creating build/bdist.linux-x86_64/egg/keras_contrib/backend\n",
            "copying build/lib/keras_contrib/backend/cntk_backend.py -> build/bdist.linux-x86_64/egg/keras_contrib/backend\n",
            "copying build/lib/keras_contrib/backend/tensorflow_backend.py -> build/bdist.linux-x86_64/egg/keras_contrib/backend\n",
            "copying build/lib/keras_contrib/backend/__init__.py -> build/bdist.linux-x86_64/egg/keras_contrib/backend\n",
            "copying build/lib/keras_contrib/backend/numpy_backend.py -> build/bdist.linux-x86_64/egg/keras_contrib/backend\n",
            "copying build/lib/keras_contrib/backend/theano_backend.py -> build/bdist.linux-x86_64/egg/keras_contrib/backend\n",
            "byte-compiling build/bdist.linux-x86_64/egg/keras_contrib/utils/test_utils.py to test_utils.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/keras_contrib/utils/__init__.py to __init__.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/keras_contrib/utils/save_load_utils.py to save_load_utils.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/keras_contrib/utils/conv_utils.py to conv_utils.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/keras_contrib/losses/__init__.py to __init__.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/keras_contrib/losses/jaccard.py to jaccard.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/keras_contrib/losses/dssim.py to dssim.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/keras_contrib/losses/crf_losses.py to crf_losses.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/keras_contrib/metrics/crf_accuracies.py to crf_accuracies.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/keras_contrib/metrics/__init__.py to __init__.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/keras_contrib/optimizers/yogi.py to yogi.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/keras_contrib/optimizers/__init__.py to __init__.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/keras_contrib/optimizers/padam.py to padam.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/keras_contrib/optimizers/ftml.py to ftml.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/keras_contrib/optimizers/lars.py to lars.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/keras_contrib/preprocessing/__init__.py to __init__.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/keras_contrib/regularizers/__init__.py to __init__.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/keras_contrib/constraints/clip.py to clip.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/keras_contrib/constraints/__init__.py to __init__.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/keras_contrib/__init__.py to __init__.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/keras_contrib/activations/squash.py to squash.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/keras_contrib/activations/__init__.py to __init__.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/keras_contrib/datasets/__init__.py to __init__.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/keras_contrib/datasets/conll2000.py to conll2000.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/keras_contrib/datasets/pascal_voc.py to pascal_voc.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/keras_contrib/datasets/coco.py to coco.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/keras_contrib/initializers/__init__.py to __init__.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/keras_contrib/initializers/convaware.py to convaware.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/keras_contrib/tests/activations.py to activations.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/keras_contrib/tests/__init__.py to __init__.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/keras_contrib/tests/optimizers.py to optimizers.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/keras_contrib/tests/metrics.py to metrics.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/keras_contrib/tests/regularizers.py to regularizers.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/keras_contrib/layers/advanced_activations/swish.py to swish.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/keras_contrib/layers/advanced_activations/sinerelu.py to sinerelu.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/keras_contrib/layers/advanced_activations/srelu.py to srelu.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/keras_contrib/layers/advanced_activations/__init__.py to __init__.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/keras_contrib/layers/advanced_activations/pelu.py to pelu.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/keras_contrib/layers/__init__.py to __init__.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/keras_contrib/layers/normalization/instancenormalization.py to instancenormalization.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/keras_contrib/layers/normalization/__init__.py to __init__.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/keras_contrib/layers/normalization/groupnormalization.py to groupnormalization.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/keras_contrib/layers/core.py to core.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/keras_contrib/layers/crf.py to crf.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/keras_contrib/layers/capsule.py to capsule.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/keras_contrib/layers/convolutional/__init__.py to __init__.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/keras_contrib/layers/convolutional/cosineconvolution2d.py to cosineconvolution2d.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/keras_contrib/layers/convolutional/subpixelupscaling.py to subpixelupscaling.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/keras_contrib/applications/densenet.py to densenet.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/keras_contrib/applications/__init__.py to __init__.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/keras_contrib/applications/resnet.py to resnet.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/keras_contrib/applications/nasnet.py to nasnet.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/keras_contrib/applications/wide_resnet.py to wide_resnet.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/keras_contrib/wrappers/__init__.py to __init__.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/keras_contrib/callbacks/cyclical_learning_rate.py to cyclical_learning_rate.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/keras_contrib/callbacks/__init__.py to __init__.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/keras_contrib/callbacks/dead_relu_detector.py to dead_relu_detector.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/keras_contrib/callbacks/tensorboard.py to tensorboard.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/keras_contrib/callbacks/snapshot.py to snapshot.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/keras_contrib/backend/cntk_backend.py to cntk_backend.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/keras_contrib/backend/tensorflow_backend.py to tensorflow_backend.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/keras_contrib/backend/__init__.py to __init__.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/keras_contrib/backend/numpy_backend.py to numpy_backend.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/keras_contrib/backend/theano_backend.py to theano_backend.cpython-36.pyc\n",
            "creating build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying tf_keras_contrib.egg-info/PKG-INFO -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying tf_keras_contrib.egg-info/SOURCES.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying tf_keras_contrib.egg-info/dependency_links.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying tf_keras_contrib.egg-info/requires.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying tf_keras_contrib.egg-info/top_level.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "zip_safe flag not set; analyzing archive contents...\n",
            "creating dist\n",
            "creating 'dist/tf_keras_contrib-2.0.8-py3.6.egg' and adding 'build/bdist.linux-x86_64/egg' to it\n",
            "removing 'build/bdist.linux-x86_64/egg' (and everything under it)\n",
            "Processing tf_keras_contrib-2.0.8-py3.6.egg\n",
            "Copying tf_keras_contrib-2.0.8-py3.6.egg to /usr/local/lib/python3.6/dist-packages\n",
            "Adding tf-keras-contrib 2.0.8 to easy-install.pth file\n",
            "\n",
            "Installed /usr/local/lib/python3.6/dist-packages/tf_keras_contrib-2.0.8-py3.6.egg\n",
            "Processing dependencies for tf-keras-contrib==2.0.8\n",
            "Finished processing dependencies for tf-keras-contrib==2.0.8\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q_ihsALFkr3w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow.keras.backend as K\n",
        "from tensorflow.keras.models import Model, Sequential\n",
        "from tensorflow.keras.layers import Layer, InputSpec, DepthwiseConv2D, Conv2D, BatchNormalization, Add, ReLU, LeakyReLU, ZeroPadding2D, Activation, LeakyReLU\n",
        "import tensorflow_addons as tfa\n",
        "\n",
        "import os\n",
        "import gc\n",
        "from glob import glob\n",
        "from itertools import product\n",
        "from random import choice\n",
        "\n",
        "from imageio import imwrite\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from tensorflow.keras.applications import VGG19\n",
        "\n",
        "import gc"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pfj_Aj-2xoun",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "Not sure what this does\n",
        "\"\"\"\n",
        "\n",
        "class ReflectionPadding2D(Layer):\n",
        "  def __init__(self, padding=(1, 1), **kwargs):\n",
        "    super(ReflectionPadding2D, self).__init__(**kwargs)\n",
        "    padding = tuple(padding)\n",
        "    self.padding = ((0, 0), padding, padding, (0, 0))\n",
        "    self.input_spec = [InputSpec(ndim=4)]\n",
        "\n",
        "  def compute_output_shape(self, s):\n",
        "    return s[0], s[1] + 2 * self.padding[0], s[2] + 2 * self.padding[1], s[3]\n",
        "\n",
        "  def call(self, x):\n",
        "    return tf.pad(x, self.padding, \"REFLECT\")"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jK9cK5baxVPX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_padding(pad_type, padding):\n",
        "  if pad_type == 'reflect':\n",
        "    return ReflectionPadding2D(padding)\n",
        "  elif pad_type == 'constant':\n",
        "    return ZeroPadding2D(padding)\n",
        "  else:\n",
        "    raise ValueError(f'Invalid padding type: {pad_type}')"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bz0hHKqpzh_P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_norm(norm_type):\n",
        "  if norm_type == 'instance':\n",
        "    return tfa.layers.InstanceNormalization()\n",
        "  elif norm_type == 'batch':\n",
        "    return BatchNormalization()\n",
        "  else:\n",
        "    raise ValueError(f'Invalid norm type: {norm_type}')"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_iZX8QK3uHXw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class FlatConv(Model):\n",
        "  def __init__(self,\n",
        "               filters,\n",
        "               kernel_size,\n",
        "               norm_type='instance',\n",
        "               pad_type='constant',\n",
        "               **kwargs):\n",
        "    super(FlatConv, self).__init__(name='FlatConv')\n",
        "    \"\"\"\n",
        "    # assuming stride=1, remaining size will be (W - kernel_size) + 1 * (8 - kernel_size) + 1 so must add back (kernel_size - 1) // 2\n",
        "    \"\"\"\n",
        "    padding = (kernel_size - 1) // 2\n",
        "    padding = (padding, padding)\n",
        "\n",
        "    self.model = Sequential([\n",
        "                             get_padding(pad_type, padding),\n",
        "                             Conv2D(filters, kernel_size),\n",
        "                             get_norm(norm_type),\n",
        "                             ReLU()\n",
        "    ])\n",
        "\n",
        "  def call(self, x, training=False):\n",
        "    return self.model(x, training=training)\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nE0WxKQf13Rn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ConvBlock(Model):\n",
        "  def __init__(self,\n",
        "               filters,\n",
        "               kernel_size,\n",
        "               stride=1,\n",
        "               norm_type='instance',\n",
        "               pad_type='constant',\n",
        "               **kwargs):\n",
        "    super(ConvBlock, self).__init__(name='ConvBlock')\n",
        "  \n",
        "    padding = (kernel_size - 1) // 2\n",
        "    padding = (padding, padding)\n",
        "\n",
        "    self.model = Sequential([\n",
        "                              get_padding(pad_type, padding),\n",
        "                              Conv2D(filters, kernel_size, strides=stride),\n",
        "                              get_padding(pad_type, padding),\n",
        "                              Conv2D(filters, kernel_size),\n",
        "                              get_norm(norm_type),\n",
        "                              ReLU()\n",
        "    ])\n",
        "\n",
        "  def call(self, x, training=False):\n",
        "    return self.model(x, training=training)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "la4ZE9gc2FXC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ResBlock(Model):\n",
        "  def __init__(self,\n",
        "               filters,\n",
        "               kernel_size,\n",
        "               norm_type='instance',\n",
        "               pad_type='constant',\n",
        "               **kwargs):\n",
        "    super(ResBlock, self).__init__(name='ResBlock')\n",
        "    \n",
        "    padding = (kernel_size - 1) // 2\n",
        "    padding = (padding, padding)\n",
        "\n",
        "    self.model = Sequential([\n",
        "                             get_padding(pad_type, padding),\n",
        "                             Conv2D(filters, kernel_size),\n",
        "                             get_norm(norm_type),\n",
        "                             ReLU(),\n",
        "                             get_padding(pad_type, padding),\n",
        "                             Conv2D(filters, kernel_size),\n",
        "                             get_norm(norm_type)\n",
        "    ])\n",
        "\n",
        "    self.add = Add()\n",
        "\n",
        "  def call(self, x, training=False):\n",
        "    return self.add([self.model(x, training=training), x])"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qMYs6f90E3cr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class UpSampleConv(Model):\n",
        "  def __init__(self,\n",
        "               filters,\n",
        "               kernel_size,\n",
        "               norm_type='instance',\n",
        "               pad_type='constant',\n",
        "               **kwargs):\n",
        "    super(UpSampleConv, self).__init__(name='UpSampleConv')\n",
        "    self.model = ConvBlock(filters, kernel_size, 1, norm_type, pad_type)\n",
        "\n",
        "  def call(self, x, training=False):\n",
        "    \"\"\"\n",
        "    In downsampling, we can get away with reducing size in half by setting stride=2\n",
        "    In upsample, we need to do directly resize the images\n",
        "    \"\"\"\n",
        "    x = tf.keras.backend.resize_images(x, 2, 2, \"channels_last\", 'bilinear')\n",
        "    return self.model(x, training=training)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3oewDUupJMvC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Generator(Model):\n",
        "  def __init__(self,\n",
        "               norm_type='instance',\n",
        "               pad_type='constant',\n",
        "               base_filters=64,\n",
        "               num_resblocks=8):\n",
        "    super(Generator, self).__init__(name='Generator')\n",
        "    self.flat_conv = FlatConv(filters=base_filters, kernel_size=7, norm_type=norm_type, pad_type=pad_type)\n",
        "    self.down_conv1 = ConvBlock(mid_filters=base_filters, filters=base_filters * 2, kernel_size=3, stride=2, norm_type=norm_type, pad_type=pad_type)\n",
        "    self.down_conv2 = ConvBlock(mid_filters=base_filters, filters=base_filters * 4, kernel_size=3, stride=2, norm_type=norm_type, pad_type=pad_type)\n",
        "    self.res_blocks = Sequential([ResBlock(filters=base_filters * 4, kernel_size=3, norm_type=norm_type, pad_type=pad_type) for _ in range(num_resblocks)])\n",
        "    self.up_conv1 = UpSampleConv(filters=base_filters * 2, kernel_size=3, norm_type=norm_type, pad_type=pad_type)\n",
        "    self.up_conv2 = UpSampleConv(filters=base_filters, kernel_size=3, norm_type=norm_type, pad_type=pad_type)\n",
        "\n",
        "    padding = (3, 3)\n",
        "    self.final_conv = Sequential([\n",
        "                                  get_padding(pad_type, padding),\n",
        "                                  Conv2D(filters=3, kernel_size=7),\n",
        "                                  Activation('tanh')\n",
        "    ])\n",
        "\n",
        "  def call(self, x, training=False):\n",
        "    x = self.flat_conv(x, training=training)\n",
        "    x = self.down_conv1(x, training=training)\n",
        "    x = self.down_conv2(x, training=training)\n",
        "    x = self.res_blocks(x, training=training)\n",
        "    x = self.up_conv1(x, training=training)\n",
        "    x = self.up_conv2(x, training=training)\n",
        "    x = self.final_conv(x, training=training)\n",
        "    return x\n",
        "\n",
        "  def compute_output_shape(self, input_shape):\n",
        "    return tf.TensorShape(input_shape)\n"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fHXDGNumInDM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class StridedConv(Model):\n",
        "  def __init__(self,\n",
        "               filters=64,\n",
        "               lrelu_alpha=0.2,\n",
        "               pad_type='constant',\n",
        "               norm_type='batch',\n",
        "               **kwargs):\n",
        "    super(StridedConv, self).__init__(name='StridedConv')\n",
        "\n",
        "    self.model = Sequential([\n",
        "                             get_padding(pad_type, (1, 1)),\n",
        "                             Conv2D(filters, 3, strides=(2, 2)),\n",
        "                             LeakyReLU(lrelu_alpha),\n",
        "                             get_padding(pad_type, (1, 1)),\n",
        "                             Conv2D(filters * 2, 3),\n",
        "                             get_norm(norm_type),\n",
        "                             LeakyReLU(lrelu_alpha)\n",
        "    ])\n",
        "\n",
        "  def call(self, x, training=False):\n",
        "    return self.model(x, training=training)\n"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7kFbWRkqPg92",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Discriminator(Model):\n",
        "  def __init__(self,\n",
        "               base_filters=32,\n",
        "               lrelu_alpha=0.2,\n",
        "               pad_type='reflect',\n",
        "               norm_type='batch'\n",
        "               ):\n",
        "    super(Discriminator, self).__init__(name='Discriminator')\n",
        "    if pad_type =='reflect':\n",
        "      self.flat_pad = ReflectionPadding2D()\n",
        "    elif pad_type == 'constant':\n",
        "      self.flat_pad = ZeroPadding2D()\n",
        "    else:\n",
        "      raise ValueError(f'Invalid pad_type {pad_type}')\n",
        "\n",
        "    self.flat_conv = Conv2D(base_filters, 3)\n",
        "    self.flat_lru = LeakyReLU(lrelu_alpha)\n",
        "    self.strided_conv1 = StridedConv(base_filters * 2, lrelu_alpha, pad_type, norm_type)\n",
        "    self.strided_conv2 = StridedConv(base_filters * 4, lrelu_alpha, pad_type, norm_type)\n",
        "    self.conv2 = Conv2D(base_filters * 8, 3)\n",
        "\n",
        "    if norm_type == 'instance':\n",
        "      self.norm = InstanceNormalization()\n",
        "    elif norm_type == 'batch':\n",
        "      self.norm = BatchNormalization()\n",
        "\n",
        "    self.lrelu = LeakyReLU(lrelu_alpha)\n",
        "    self.final_conv = Conv2D(1, 3)\n",
        "\n",
        "  def call(self, x, training=False):\n",
        "    x = self.flat_pad(x, training=training)\n",
        "    x = self.flat_conv(x, training=training)\n",
        "    x = self.flat_lru(x, training=training)\n",
        "    x = self.strided_conv1(x, training=training)\n",
        "    x = self.strided_conv2(x, training=training)\n",
        "    x = self.conv2(x, training=training)\n",
        "    x = self.norm(x, training=training)\n",
        "    x = self.lrelu(x, training=training)\n",
        "    x = self.final_conv(x, training=training)\n",
        "    return x"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vt0h1_3iPh1s",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 532
        },
        "outputId": "7b3ba4f2-bafa-4067-ed76-faf07fa6e4a0"
      },
      "source": [
        "shape = (1, 256, 256, 3)\n",
        "nx = np.random.rand(*shape).astype(np.float32)\n",
        "t = tf.keras.Input(shape=nx.shape[1:], batch_size=nx.shape[0])\n",
        "print(nx.shape)\n",
        "d = Discriminator()\n",
        "out = d(t)\n",
        "d.summary()\n",
        "print(f\"Input  Shape: {t.shape}\")\n",
        "print(f\"Output Shape: {out.shape}\")"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1, 256, 256, 3)\n",
            "Model: \"Discriminator\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "reflection_padding2d (Reflec (1, 258, 258, 3)          0         \n",
            "_________________________________________________________________\n",
            "conv2d (Conv2D)              (1, 256, 256, 32)         896       \n",
            "_________________________________________________________________\n",
            "leaky_re_lu (LeakyReLU)      (1, 256, 256, 32)         0         \n",
            "_________________________________________________________________\n",
            "StridedConv (StridedConv)    (1, 128, 128, 128)        92864     \n",
            "_________________________________________________________________\n",
            "StridedConv (StridedConv)    (1, 64, 64, 256)          443776    \n",
            "_________________________________________________________________\n",
            "conv2d_5 (Conv2D)            (1, 62, 62, 256)          590080    \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (1, 62, 62, 256)          1024      \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_5 (LeakyReLU)    (1, 62, 62, 256)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_6 (Conv2D)            (1, 60, 60, 1)            2305      \n",
            "=================================================================\n",
            "Total params: 1,130,945\n",
            "Trainable params: 1,129,665\n",
            "Non-trainable params: 1,280\n",
            "_________________________________________________________________\n",
            "Input  Shape: (1, 256, 256, 3)\n",
            "Output Shape: (1, 60, 60, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aRQIMlR3POcC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def vgg_model(input_size):\n",
        "  input_shape = (input_size, input_size, 3)\n",
        "  base_model = VGG19(weights=\"imagenet\", include_top=False, input_shape=input_shape)\n",
        "  conv_block = base_model.get_layer('block4_conv3').output\n",
        "  new_conv_block = Conv2D(512, (3, 3), activation='linear', padding='same', name='block4_conv4')(conv_block)\n",
        "  vgg = Model(inputs=base_model.input, outputs=new_conv_block)\n",
        "  vgg.load_weights(os.path.expanduser(os.path.join(\"~\", \".keras\", \"models\", \"vgg19_weights_tf_dim_ordering_tf_kernels_notop.h5\")), by_name=True)\n",
        "  return vgg"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QhDdS3qpNKCD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def gram_matrix(x):\n",
        "  # Do not really understand these details but they ultimately build the gram matrix so we can add texture details from the style image onto content image from the VGG output\n",
        "  shape_x = tf.shape(x)\n",
        "  b = shape_x[0]\n",
        "  c = shape_x[3]\n",
        "  x = tf.reshape(x, [b, -1, c])\n",
        "  return tf.matmul(tf.transpose(x, [0, 2, 1]), x) / tf.cast((tf.size(x) // b), tf.float32)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rtB28wCmLwOM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def content_loss(input_images, generated_images):\n",
        "  return mae(input_images, generated_images)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h0LDWKImL-P9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def style_loss(input_images, generated_images):\n",
        "  return mae(gram_matrix(input_images), gram_matrix(generated_images))"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d7W15hI_Hoo4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def discriminator_loss(real_output, fake_output, smooth_output):\n",
        "  # real_loss = d_loss_obj([1 for _ in range(real_output)], real_output)\n",
        "  # fake_loss = d_loss_obj([0 for _ in range(fake_output)], fake_output)\n",
        "  # smooth_loss = d_loss_obj([0 for _ in range(smooth_output)], smooth_output)\n",
        "  real_loss = d_loss_obj(tf.ones_like(real_output), real_output)\n",
        "  fake_loss = d_loss_obj(tf.zeros_like(fake_output), fake_output)\n",
        "  smooth_loss = d_loss_obj(tf.zeros_like(smooth_output), smooth_output)\n",
        "  total_loss = real_loss + fake_loss + smooth_loss\n",
        "  return real_loss, fake_loss, smooth_loss, total_loss"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QXiotLRNKGfT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generator_adversarial_loss(fake_output):\n",
        "  # return g_loss_obj([1 for _ in range(fake_output)], fake_output)\n",
        "  return g_loss_obj(tf.ones_like(fake_output), fake_output)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uwlpFgkRS7Nb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generator_loss(source_images, target_images, generated_images, fake_output):\n",
        "    g_adv_loss = g_adv_lambda * generator_adversarial_loss(fake_output)\n",
        "    vgg_generated_images = vgg(generated_images)\n",
        "    c_loss = content_lambda * content_loss(vgg(source_images), vgg_generated_images)\n",
        "    s_loss = style_lambda * style_loss(vgg(target_images[:vgg_generated_images.shape[0]]), vgg_generated_images)\n",
        "    g_total_loss = g_adv_loss + c_loss + s_loss\n",
        "    return g_adv_loss, g_total_loss"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2oti_qy3lGwS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_step(source_images, target_images, smooth_images, generator, discriminator, g_optimizer, d_optimizer):\n",
        "  with tf.GradientTape() as g_tape, tf.GradientTape() as d_tape:\n",
        "\n",
        "    print(f'Get output')\n",
        "    # get output\n",
        "    real_output = discriminator(target_images, training=True)\n",
        "    generated_images = generator(source_images, training=True)\n",
        "    fake_output = discriminator(generated_images, training=True)\n",
        "    smooth_output = discriminator(smooth_images, training=True)\n",
        "\n",
        "    print(f'Calculate loss')\n",
        "    # calculate loss\n",
        "    d_real_loss, d_fake_loss, d_smooth_loss, d_total_loss = discriminator_loss(real_output, fake_output, smooth_output)\n",
        "    g_adv_loss, g_total_loss = generator_loss(source_images, target_images, generated_images, fake_output)\n",
        "\n",
        "    print(f'Calculate gradient')\n",
        "    # calculate gradient\n",
        "    d_grads = d_tape.gradient(d_total_loss, discriminator.trainable_variables)\n",
        "    g_grads = g_tape.gradient(g_total_loss, generator.trainable_variables)\n",
        "\n",
        "    print(f'Apply gradients')\n",
        "    # apply gradients with optimizer\n",
        "    d_optimizer.apply_gradients(zip(d_grads, discriminator.trainable_variables))\n",
        "    g_optimizer.apply_gradients(zip(g_grads, generator.trainable_variables))\n"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5LUvi2LKYHjn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def random_resize(x):\n",
        "  size = choice(sizes)\n",
        "  return tf.image.resize(x, (size, size))\n",
        "\n",
        "def image_processing(file_name, is_train=True):\n",
        "  crop_size = input_size\n",
        "\n",
        "  if is_train:\n",
        "    crop_size += 32\n",
        "\n",
        "  img = tf.io.read_file(file_name)\n",
        "  img = tf.image.decode_jpeg(img, channels=3)\n",
        "\n",
        "  if is_train:\n",
        "    sizes = tf.cast(crop_size * tf.random.uniform([2], 0.9, 1.1), tf.int32)\n",
        "    shape = tf.shape(img)[:2]\n",
        "    sizes = tf.minimum(sizes, shape)\n",
        "    img = tf.image.random_crop(img, (sizes[0], sizes[1], 3))\n",
        "    img = tf.image.random_flip_left_right(img)\n",
        "  \n",
        "  img = tf.image.resize(img, (crop_size, crop_size))\n",
        "  img = tf.cast(img, tf.float32) / 127.5 - 1\n",
        "  return img\n",
        "\n",
        "def preprocess(file_name):\n",
        "  return random_resize(image_processing(file_name, True))\n",
        "\n",
        "def get_dataset(dataset_name, domain, _type, batch_size):\n",
        "  files = glob(os.path.join(data_dir, dataset_name, f'{_type}{domain}', '*'))\n",
        "  num_files = len(files)\n",
        "  dataset = tf.data.Dataset.from_tensor_slices(files)\n",
        "  dataset = dataset.apply(tf.data.experimental.shuffle_and_repeat(num_files))\n",
        "  dataset = dataset.apply(tf.data.experimental.map_and_batch(preprocess, batch_size))\n",
        "  steps = int(np.ceil(num_files / batch_size))\n",
        "  return iter(dataset), steps"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xYdnkk1pUXVr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(generator=None):\n",
        "  dataset_source, steps_per_epoch = get_dataset(dataset_name=dataset_name, domain=source_domain, _type='train', batch_size=batch_size)\n",
        "  dataset_target, _ = get_dataset(dataset_name=dataset_name, domain=target_domain, _type='train', batch_size=batch_size)\n",
        "  dataset_smooth, _ = get_dataset(dataset_name=dataset_name, domain=smooth_domain, _type='train', batch_size=batch_size)\n",
        "\n",
        "  g_optimizer = tf.keras.optimizers.Adam(learning_rate=1e-5, beta_1=.5)\n",
        "  d_optimizer = tf.keras.optimizers.Adam(learning_rate=1e-5, beta_1=.5)\n",
        "\n",
        "  if generator is None:\n",
        "    generator = Generator(base_filters=64)\n",
        "    generator(tf.keras.Input(shape=(input_size, input_size, 3), batch_size=batch_size))\n",
        "\n",
        "  discriminator = Discriminator(base_filters=32)\n",
        "  discriminator(tf.keras.Input(shape=(input_size, input_size, 3), batch_size=batch_size))\n",
        "\n",
        "  # save info\n",
        "  d_checkpoint = tf.train.Checkpoint(discriminator=discriminator)\n",
        "  g_checkpoint = tf.train.Checkpoint(generator=generator)\n",
        "\n",
        "  steps_per_epoch = 3 # tempo\n",
        "\n",
        "  for epoch in range(epochs):\n",
        "    for step in tqdm(range(steps_per_epoch), desc=f'Train {epoch} / {epochs}', total=steps_per_epoch):\n",
        "      source_images = dataset_source.next()\n",
        "      target_images = dataset_source.next()\n",
        "      smooth_images = dataset_source.next()\n",
        "\n",
        "      print(f'Step: {step}')\n",
        "      train_step(source_images, target_images, smooth_images, generator, discriminator, g_optimizer, d_optimizer)\n",
        "\n",
        "    d_checkpoint.save(file_prefix='discriminator')\n",
        "    g_checkpoint.save(file_prefix='generator')\n",
        "\n",
        "    discriminator.save_weights(os.path.join(model_dir, 'discriminator'))\n",
        "    generator.save_weights(os.path.join(model_dir, 'generator'))\n",
        "    gc.collect()\n",
        "\n",
        "  del dataset_source, dataset_target, dataset_smooth\n",
        "  gc.collect()"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uQHVCBPAQbmX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def pretrain_step(input_images, generator, optimizer):\n",
        "  with tf.GradientTape() as tape:\n",
        "    # get output\n",
        "    generated_images = generator(input_images)\n",
        "\n",
        "    # calculate loss\n",
        "    vgg_generated_images = vgg(generated_images)\n",
        "    loss = content_lambda * content_loss(vgg(input_images), vgg_generated_images)\n",
        "\n",
        "    # calculate gradient\n",
        "    grads = tape.gradient(loss, generator.trainable_variables)\n",
        "\n",
        "    # apply gradient\n",
        "    optimizer.apply_gradients(zip(grads, generator.trainable_variables))"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tu1DcCaQBWgy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def pretrain_generator():\n",
        "  dataset_source, steps_per_epoch = get_dataset(dataset_name=dataset_name, domain=source_domain, _type='train', batch_size=batch_size)\n",
        "\n",
        "  optimizer = tf.keras.optimizers.Adam(learning_rate=1e-5, beta_1=.5)\n",
        "\n",
        "  generator = Generator(base_filters=64)\n",
        "  generator(tf.keras.Input(shape=(input_size, input_size, 3), batch_size=batch_size))\n",
        "\n",
        "  # save info\n",
        "  checkpoint = tf.train.Checkpoint(generator=generator)\n",
        "\n",
        "  steps_per_epoch = 3 # temp\n",
        "\n",
        "  for epoch in range(epochs):\n",
        "    for step in tqdm(range(steps_per_epoch), desc=f'Train {epoch} / {epochs}', total=steps_per_epoch):\n",
        "      source_images = dataset_source.next()\n",
        "\n",
        "      pretrain_step(source_images, generator, optimizer)\n",
        "\n",
        "    checkpoint.save(file_prefix='generator')\n",
        "    generator.save_weights(os.path.join(model_dir, 'generator'))\n",
        "    gc.collect() # collect garbage often\n",
        "\n",
        "  del dataset_source      \n",
        "\n",
        "  gc.collect()\n",
        "\n",
        "  return generator"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KAMZZdsmIiDB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "d_loss_obj = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
        "g_loss_obj = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
        "mae = tf.keras.losses.MeanAbsoluteError()\n",
        "g_adv_lambda = 1.\n",
        "content_lambda = 10.\n",
        "style_lambda = 1.\n",
        "input_size = 256\n",
        "sizes = [input_size - 32, input_size, input_size + 32]\n",
        "vgg = vgg_model(input_size)\n",
        "epochs = 1\n",
        "dataset_name = '/content/drive/My Drive/pg-toons/naruto_aladdin_small'\n",
        "data_dir = './'\n",
        "source_domain = 'A'\n",
        "target_domain = 'B'\n",
        "smooth_domain = 'B_smooth'\n",
        "batch_size = 1\n",
        "model_dir = '/content/drive/My Drive/pg-toons/my_models'"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2tRiSwaiYAnr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 478
        },
        "outputId": "5d853bf5-1eb3-4fd9-a05d-c87e3543a590"
      },
      "source": [
        "generator = pretrain_generator()\n",
        "train(generator)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train 0 / 1: 100%|| 3/3 [00:01<00:00,  2.86it/s]\n",
            "Train 0 / 1:   0%|          | 0/3 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Step: 0\n",
            "Get output\n",
            "Calculate loss\n",
            "WARNING:tensorflow:Model was constructed with shape (None, 256, 256, 3) for input Tensor(\"input_7:0\", shape=(None, 256, 256, 3), dtype=float32), but it was called on an input with incompatible shape (1, 288, 288, 3).\n",
            "WARNING:tensorflow:Model was constructed with shape (None, 256, 256, 3) for input Tensor(\"input_7:0\", shape=(None, 256, 256, 3), dtype=float32), but it was called on an input with incompatible shape (1, 288, 288, 3).\n",
            "WARNING:tensorflow:Model was constructed with shape (None, 256, 256, 3) for input Tensor(\"input_7:0\", shape=(None, 256, 256, 3), dtype=float32), but it was called on an input with incompatible shape (1, 288, 288, 3).\n",
            "Calculate gradient\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rTrain 0 / 1:  33%|      | 1/3 [00:01<00:03,  1.54s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Apply gradients\n",
            "Step: 1\n",
            "Get output\n",
            "Calculate loss\n",
            "WARNING:tensorflow:Model was constructed with shape (None, 256, 256, 3) for input Tensor(\"input_7:0\", shape=(None, 256, 256, 3), dtype=float32), but it was called on an input with incompatible shape (1, 288, 288, 3).\n",
            "WARNING:tensorflow:Model was constructed with shape (None, 256, 256, 3) for input Tensor(\"input_7:0\", shape=(None, 256, 256, 3), dtype=float32), but it was called on an input with incompatible shape (1, 288, 288, 3).\n",
            "WARNING:tensorflow:Model was constructed with shape (None, 256, 256, 3) for input Tensor(\"input_7:0\", shape=(None, 256, 256, 3), dtype=float32), but it was called on an input with incompatible shape (1, 288, 288, 3).\n",
            "Calculate gradient\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rTrain 0 / 1:  67%|   | 2/3 [00:02<00:01,  1.24s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Apply gradients\n",
            "Step: 2\n",
            "Get output\n",
            "Calculate loss\n",
            "WARNING:tensorflow:Model was constructed with shape (None, 256, 256, 3) for input Tensor(\"input_7:0\", shape=(None, 256, 256, 3), dtype=float32), but it was called on an input with incompatible shape (1, 288, 288, 3).\n",
            "WARNING:tensorflow:Model was constructed with shape (None, 256, 256, 3) for input Tensor(\"input_7:0\", shape=(None, 256, 256, 3), dtype=float32), but it was called on an input with incompatible shape (1, 288, 288, 3).\n",
            "WARNING:tensorflow:Model was constructed with shape (None, 256, 256, 3) for input Tensor(\"input_7:0\", shape=(None, 256, 256, 3), dtype=float32), but it was called on an input with incompatible shape (1, 288, 288, 3).\n",
            "Calculate gradient\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Train 0 / 1: 100%|| 3/3 [00:02<00:00,  1.10it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Apply gradients\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1mDPFgHwcv4w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}